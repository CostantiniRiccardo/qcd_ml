{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c224f9a-3545-4c61-baee-17554a51e60c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c30c60da-9d26-4df3-8b68-02f8c3482e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_160777/2789506356.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  U_smeared = torch.load(\"U_smeared.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"src/\")\n",
    "\n",
    "from qcd_ml.nn.lptc import v_LPTC\n",
    "from qcd_ml.qcd.dirac import dirac_wilson_clover\n",
    "#from qcd_ml.compat.gpt import lattice2ndarray, ndarray2lattice\n",
    "from qcd_ml.util.solver import GMRES_torch\n",
    "from qcd_ml.util.qcd.multigrid import ZPP_Multigrid\n",
    "from qcd_ml.base.paths import v_evaluate_path, v_ng_evaluate_path, v_reverse_evaluate_path, PathBuffer\n",
    "from qcd_ml.base.operations import v_spin_transform, v_ng_spin_transform, v_gauge_transform\n",
    "\n",
    "\n",
    "vec = torch.complex(\n",
    "        torch.randn(8, 8, 8, 16, 4, 3, dtype=torch.double)\n",
    "        , torch.randn(8, 8, 8, 16, 4, 3, dtype=torch.double))\n",
    "\n",
    "U = torch.tensor(np.load(os.path.join(\"test\", \"assets\",\"1500.config.npy\")))\n",
    "\n",
    "U_smeared = torch.load(\"U_smeared.pt\")\n",
    "U_smeared = [torch.tensor(np.array(Us)) for Us in U_smeared]\n",
    "\n",
    "import itertools\n",
    "def get_paths_lexicographic(block_size):\n",
    "    paths = []\n",
    "    for position in itertools.product(*(range(bs) for bs in block_size)):\n",
    "        path = sum([[(mu, -1)] for mu, n in enumerate(position) for _ in range(n)], start=[])\n",
    "        paths.append(path)\n",
    "    return paths\n",
    "\n",
    "def get_paths_reverse_lexicographic(block_size):\n",
    "    return [list(reversed(pth)) for pth in get_paths_lexicographic(block_size)]\n",
    "\n",
    "def get_paths_one_step_lexicographic(block_size):\n",
    "    paths = []\n",
    "    for position in itertools.product(*(range(bs) for bs in block_size)):\n",
    "        path = []\n",
    "        pos = np.array(position)\n",
    "        while pos.any():\n",
    "            for mu in range(pos.shape[0]):\n",
    "                if pos[mu] > 0:\n",
    "                    path.append((mu, -1))\n",
    "                    pos[mu] -= 1\n",
    "        paths.append(path)\n",
    "    return paths\n",
    "def get_paths_one_step_reverse_lexicographic(block_size):\n",
    "    return [list(reversed(pth)) for pth in get_paths_one_step_lexicographic(block_size)]\n",
    "\n",
    "\n",
    "def path_get_orig_point(path):\n",
    "    point = [0] * 4\n",
    "    for mu, nhops in path:\n",
    "        point[mu] -= nhops\n",
    "    return point\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0f34ef8-e571-4499-adbd-92a746fc503d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1946293122.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 16\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class v_ProjectLayer(torch.nn.Module):\n",
    "    def __init__(self, gauges_and_paths, L_fine, L_coarse):\n",
    "        super().__init__()\n",
    "        self.path_buffers = [[PathBuffer(Ui, pij) for pij in pi] for Ui, pi in gauges_and_paths]\n",
    "        self.weights = torch.nn.Parameter(\n",
    "                torch.randn(len(gauges_and_paths), *tuple(U.shape[1:-2]), 4, 4, dtype=torch.cdouble)\n",
    "                )\n",
    "        self.L_fine = L_fine\n",
    "        self.L_coarse = L_coarse\n",
    "        self.block_size = [lf // lc for lf, lc in zip(L_fine, L_coarse)]\n",
    "\n",
    "        self.base_points = [[path_get_orig_point(pb.path) for pb in path_buffers] for path_buffers in self.path_buffers]\n",
    "        # This keeps a gauge field for every point on the lattice\n",
    "        # such that we can use this exact field to transform the\n",
    "        # base_points before summing them up.\n",
    "        self.gauge_fields = torch.zeros(len(gauges_and_paths), U.shape[1:], dtype=torch.cdouble)\n",
    "\n",
    "        identity = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=torch.cdouble)\n",
    "        for i, gpi in enumerate(self.path_buffers):\n",
    "            for j, pb in enumerate(gpi):\n",
    "                base_point = self.base_points[i,j]\n",
    "                if pb._is_identity:\n",
    "                    self.gauge_fields[i\n",
    "                            , base_point[0]::self.block_size[0]\n",
    "                            , base_point[1]::self.block_size[1]\n",
    "                            , base_point[2]::self.block_size[2]\n",
    "                            , base_point[3]::self.block_size[3]] = identity\n",
    "                else:\n",
    "                    self.gauge_fields[i\n",
    "                            , base_point[0]::self.block_size[0]\n",
    "                            , base_point[1]::self.block_size[1]\n",
    "                            , base_point[2]::self.block_size[2]\n",
    "                            , base_point[3]::self.block_size[3]] = pb.accumulated_U[base_point[0]::self.block_size[0]\n",
    "                                                                        , base_point[1]::self.block_size[1]\n",
    "                                                                        , base_point[2]::self.block_size[2]\n",
    "                                                                        , base_point[3]::self.block_size[3]]\n",
    "        def v_project(self, features_in):\n",
    "            before_pool = torch.zeros(features_in.shape[0], self.gauge_fields.shape[0], *features_in.shape[1:]\n",
    "                                     , dtype=torch.cdouble)\n",
    "            for i, fea_i in enumerate(features_in):\n",
    "                for j, (gfj, wj) in enumerate(zip(self.gauge_fields, self.weights)):\n",
    "                    before_pool[i,j] = v_spin_transform(wj, v_gauge_transform(gfj, fea_i))\n",
    "\n",
    "            # FIXME: use scatter_add_ to pool inside the block.\n",
    "            return torch.sum(before_pool, axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9051afab-db5c-460c-aa65-c7e017615f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = PathBuffer(U, [(0,-1), (1,-3), (2,-2)])\n",
    "orig_p = path_get_orig_point(pb.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b78106ef-52fb-4c37-98c3-d142e6197036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 2, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdf83c45-7c7c-4187-b13b-f2bac5017deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = torch.zeros_like(vec)\n",
    "psi[*tuple(orig_p), 0, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "402def3b-805b-4fc8-b363-5018aee82d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5171+0.3549j,  0.2084+0.2717j, -0.6081-0.3459j],\n",
       "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
       "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],\n",
       "        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j]],\n",
       "       dtype=torch.complex128)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb.v_transport(psi)[0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299ba5e-3dfb-463f-8ecd-58ed618b69f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
